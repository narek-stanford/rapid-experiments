# rapid-experiments

3 key conclusions:
1. The hidden layer size of 256 is great! Tried 128, 512, 1024..
2. ‘dropout’ probability seemed to be perfect around the value of 0.5
3. "softPLUS" activation function aced! (above, logistic/sigmoid, ReLU, softsign, TanH)


Achieved above-80% test accuracy!!
