# rapid-experiments

3 key conclusions:
1. The hidden layer size of 256 is great! Tried 128, 512, 1024..
2. ‘dropout’ probability seemed to be perfect around the value of 0.5
3. "softPLUS" activation function aced! (better than logistic/sigmoid, ReLU, softsign, TanH, in this order!)


Achieved above-80% test accuracy!!
